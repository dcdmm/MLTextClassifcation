{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
       "      <td>2</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>2</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
       "      <td>2</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>2</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>2</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "      <td>0</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>Renteria signing a top-shelf deal Red Sox gene...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>Saban not going to Dolphins yet The Miami Dolp...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>Today's NFL games PITTSBURGH at NY GIANTS Time...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>Nets get Carter from Raptors INDIANAPOLIS -- A...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label label_text\n",
       "0       Wall St. Bears Claw Back Into the Black (Reute...      2   Business\n",
       "1       Carlyle Looks Toward Commercial Aerospace (Reu...      2   Business\n",
       "2       Oil and Economy Cloud Stocks' Outlook (Reuters...      2   Business\n",
       "3       Iraq Halts Oil Exports from Main Southern Pipe...      2   Business\n",
       "4       Oil prices soar to all-time record, posing new...      2   Business\n",
       "...                                                   ...    ...        ...\n",
       "119995  Pakistan's Musharraf Says Won't Quit as Army C...      0      World\n",
       "119996  Renteria signing a top-shelf deal Red Sox gene...      1     Sports\n",
       "119997  Saban not going to Dolphins yet The Miami Dolp...      1     Sports\n",
       "119998  Today's NFL games PITTSBURGH at NY GIANTS Time...      1     Sports\n",
       "119999  Nets get Carter from Raptors INDIANAPOLIS -- A...      1     Sports\n",
       "\n",
       "[120000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lst = []\n",
    "\n",
    "with open('train.jsonl', encoding='utf-8') as fp:\n",
    "    for item in jsonlines.Reader(fp):\n",
    "        train_lst.append(item)\n",
    "train_df = pd.DataFrame(train_lst)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fears for T N pension after talks Unions repre...</td>\n",
       "      <td>2</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>3</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP) ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP) ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP) AP...</td>\n",
       "      <td>3</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>Around the world Ukrainian presidential candid...</td>\n",
       "      <td>0</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>Void is filled with Clement With the supply of...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>Martinez leaves bitter Like Roger Clemens did ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>5 of arthritis patients in Singapore take Bext...</td>\n",
       "      <td>2</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>EBay gets into rentals EBay plans to buy the a...</td>\n",
       "      <td>2</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label label_text\n",
       "0     Fears for T N pension after talks Unions repre...      2   Business\n",
       "1     The Race is On: Second Private Team Sets Launc...      3   Sci/Tech\n",
       "2     Ky. Company Wins Grant to Study Peptides (AP) ...      3   Sci/Tech\n",
       "3     Prediction Unit Helps Forecast Wildfires (AP) ...      3   Sci/Tech\n",
       "4     Calif. Aims to Limit Farm-Related Smog (AP) AP...      3   Sci/Tech\n",
       "...                                                 ...    ...        ...\n",
       "7595  Around the world Ukrainian presidential candid...      0      World\n",
       "7596  Void is filled with Clement With the supply of...      1     Sports\n",
       "7597  Martinez leaves bitter Like Roger Clemens did ...      1     Sports\n",
       "7598  5 of arthritis patients in Singapore take Bext...      2   Business\n",
       "7599  EBay gets into rentals EBay plans to buy the a...      2   Business\n",
       "\n",
       "[7600 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lst = []\n",
    "\n",
    "with open('test.jsonl', encoding='utf-8') as fp:\n",
    "    for item in jsonlines.Reader(fp):\n",
    "        test_lst.append(item)\n",
    "test_df = pd.DataFrame(test_lst)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(line):\n",
    "    \"\"\"基础英文分词器\"\"\"\n",
    "    _patterns = [r\"\\'\", r\"\\\"\", r\"\\.\", r\"<br \\/>\", r\",\", r\"\\(\", r\"\\)\", r\"\\!\", r\"\\?\", r\"\\;\", r\"\\:\", r\"\\s+\"]\n",
    "    _replacements = [\" '  \", \"\", \" . \", \" \", \" , \", \" ( \", \" ) \", \" ! \", \" ? \", \" \", \" \", \" \"]\n",
    "    _patterns_dict = list((re.compile(p), r) for p, r in zip(_patterns, _replacements))\n",
    "    line = line.lower()\n",
    "    for pattern_re, replaced_str in _patterns_dict:\n",
    "        line = pattern_re.sub(replaced_str, line)\n",
    "    return line.split()\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"\n",
    "    Vocabulary for text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens=None, min_freq=2, reserved_tokens=None):\n",
    "        # tokens: 单词tokens\n",
    "        # min_freq: The minimum frequency needed to include a token in the vocabulary.\n",
    "        # reserved_tokens: 自定义tokens\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        counter = collections.Counter(tokens)\n",
    "        # Sort according to frequencies\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)  # 未在字典中则返回'<unk>'\n",
    "        return [self.__getitem__(token) for token in tokens]  # 递归\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        \"\"\"第indices位置处的token\"\"\"\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        \"\"\"Index for the unknown token\"\"\"\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "\n",
    "split_list = []\n",
    "for _, ser in train_df.iterrows():\n",
    "    split_list.extend(tokenizer(ser['text']))\n",
    "\n",
    "vocab = Vocab(split_list, min_freq=1, reserved_tokens=['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [wall, st, ., bears, claw, back, into, the, bl...\n",
       "1         [carlyle, looks, toward, commercial, aerospace...\n",
       "2         [oil, and, economy, cloud, stocks, ', outlook,...\n",
       "3         [iraq, halts, oil, exports, from, main, southe...\n",
       "4         [oil, prices, soar, to, all-time, record, ,, p...\n",
       "                                ...                        \n",
       "119995    [pakistan, ', s, musharraf, says, won, ', t, q...\n",
       "119996    [renteria, signing, a, top-shelf, deal, red, s...\n",
       "119997    [saban, not, going, to, dolphins, yet, the, mi...\n",
       "119998    [today, ', s, nfl, games, pittsburgh, at, ny, ...\n",
       "119999    [nets, get, carter, from, raptors, indianapoli...\n",
       "Name: text, Length: 120000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split_sentence = train_df['text'].map(lambda x: tokenizer(x))\n",
    "train_split_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         29\n",
       "1         42\n",
       "2         40\n",
       "3         40\n",
       "4         43\n",
       "          ..\n",
       "119995    47\n",
       "119996    62\n",
       "119997    47\n",
       "119998    81\n",
       "119999    40\n",
       "Name: text, Length: 120000, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def com_sentence_len(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "train_sentence_len = train_split_sentence.apply(com_sentence_len)\n",
    "train_sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文本长度大部分(99.9%)141以内(可以以此进行文本最大长度截断)\n",
    "np.percentile(train_sentence_len.values, q=99.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3118881d65040a2ab1ba4879c753631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400000, 50])\n",
      "tensor([[ 0.4180,  0.2497, -0.4124,  ..., -0.1841, -0.1151, -0.7858],\n",
      "        [ 0.0134,  0.2368, -0.1690,  ..., -0.5666,  0.0447,  0.3039],\n",
      "        [ 0.1516,  0.3018, -0.1676,  ..., -0.3565,  0.0164,  0.1022],\n",
      "        ...,\n",
      "        [-0.5118,  0.0587,  1.0913,  ..., -0.2500, -1.1250,  1.5863],\n",
      "        [-0.7590, -0.4743,  0.4737,  ...,  0.7895, -0.0141,  0.6448],\n",
      "        [ 0.0726, -0.5139,  0.4728,  ..., -0.1891, -0.5902,  0.5556]])\n"
     ]
    }
   ],
   "source": [
    "class Vectors:\n",
    "    def __init__(self, name, max_vectors=None) -> None:\n",
    "        self.vectors = None\n",
    "        self.name = name\n",
    "        self.max_vectors = max_vectors\n",
    "        self.itos = None\n",
    "        self.stoi = None\n",
    "        self.cache()\n",
    "\n",
    "    def cache(self):\n",
    "        with open(self.name, \"r\", encoding='utf-8') as f:\n",
    "            read_value = f.readlines()\n",
    "\n",
    "        all_value, itos = [], []\n",
    "        for i in tqdm(range(len(read_value))):\n",
    "            l_split = read_value[i].split(' ')\n",
    "            itos.append(l_split[0])\n",
    "            all_value.append([float(i.strip()) for i in l_split[1: ]])\n",
    "        all_value = torch.tensor(all_value)\n",
    "        self.vectors = all_value\n",
    "        self.itos = itos\n",
    "        num_lines = len(self.vectors)\n",
    "        if not self.max_vectors or self.max_vectors > num_lines:\n",
    "            self.max_vectors = num_lines\n",
    "        self.vectors = self.vectors[:self.max_vectors, :]\n",
    "        self.itos = self.itos[:self.max_vectors]\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vectors)\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        if token in self.stoi:\n",
    "            return self.vectors[self.stoi[token]]\n",
    "        else:\n",
    "            dim = self.vectors.shape[1]\n",
    "            return torch.Tensor.zero_(torch.Tensor(dim))\n",
    "        \n",
    "    def get_vecs_by_tokens(self, tokens):\n",
    "        indices = [self[token] for token in tokens]\n",
    "        vecs = torch.stack(indices)\n",
    "        return vecs\n",
    "\n",
    "# 预训练词向量\n",
    "vec1 = Vectors(name=\"glove.6B.50d.txt\")\n",
    "print(vec1.vectors.shape)\n",
    "print(vec1.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1516,  0.3018, -0.1676,  ..., -0.3565,  0.0164,  0.1022],\n",
       "        ...,\n",
       "        [ 0.6241,  0.0100, -0.8427,  ...,  2.2487, -0.5393,  0.3112],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型预训练词向量矩阵\n",
    "pre_trained = vec1.get_vecs_by_tokens(vocab.idx_to_token)\n",
    "pre_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda line: [vocab[token] for token in tokenizer(line)]\n",
    "\n",
    "def truncate_pad(line, text_max_len, padding_token):\n",
    "    \"\"\"截断或填充文本序列\"\"\"\n",
    "    if len(line) > text_max_len:\n",
    "        return line[:text_max_len]  # 句子截断\n",
    "    return line + [padding_token] * (text_max_len - len(line))  # 句子填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\n",
      "[870, 12, 84, 138, 1483, 35, 174, 1752, 4058, 401, 21, 6557, 36027, 234, 68, 43, 17, 4486, 17, 35, 174, 19, 11151, 2452, 321, 195, 9804, 2]\n",
      "[870, 12, 84, 138, 1483, 35, 174, 1752, 4058, 401, 21, 6557, 36027, 234, 68, 43, 17, 4486, 17, 35, 174, 19, 11151, 2452, 321, 195, 9804, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def to_map_style_dataset(df):\n",
    "    r\"\"\"Convert DataFrame to map-style dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    class _MapStyleDataset(torch.utils.data.Dataset):\n",
    "\n",
    "        def __init__(self, df):\n",
    "            self._data = df.values\n",
    "\n",
    "        def __len__(self):\n",
    "            return self._data.shape[0]\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self._data[idx]\n",
    "\n",
    "    return _MapStyleDataset(df)\n",
    "\n",
    "\n",
    "test_map_data = to_map_style_dataset(test_df)\n",
    "for text, label, _ in test_map_data:\n",
    "    print(label)\n",
    "    print(text)\n",
    "    print(text_pipeline(text))\n",
    "    print(truncate_pad(text_pipeline(text), 30, vocab['<pad>']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\n",
      "3 The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com) SPACE.com - TORONTO, Canada -- A second\\team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket.\n",
      "3 Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.\n",
      "3 Prediction Unit Helps Forecast Wildfires (AP) AP - It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.\n",
      "3 Calif. Aims to Limit Farm-Related Smog (AP) AP - Southern California's smog-fighting agency went after emissions of the bovine variety Friday, adopting the nation's first rules to reduce air pollution from dairy cow manure.\n",
      "3 Open Letter Against British Copyright Indoctrination in Schools The British Department for Education and Skills (DfES) recently launched a \"Music Manifesto\" campaign, with the ostensible intention of educating the next generation of British musicians. Unfortunately, they also teamed up with the music industry (EMI, and various artists) to make this popular. EMI has apparently negotiated their end well, so that children in our schools will now be indoctrinated about the illegality of downloading music.The ignorance and audacity of this got to me a little, so I wrote an open letter to the DfES about it. Unfortunately, it's pedantic, as I suppose you have to be when writing to goverment representatives. But I hope you find it useful, and perhaps feel inspired to do something similar, if or when the same thing has happened in your area.\n",
      "3 Loosing the War on Terrorism \\\\\"Sven Jaschan, self-confessed author of the Netsky and Sasser viruses, is\\responsible for 70 percent of virus infections in 2004, according to a six-month\\virus roundup published Wednesday by antivirus company Sophos.\"\\\\\"The 18-year-old Jaschan was taken into custody in Germany in May by police who\\said he had admitted programming both the Netsky and Sasser worms, something\\experts at Microsoft confirmed. (A Microsoft antivirus reward program led to the\\teenager's arrest.) During the five months preceding Jaschan's capture, there\\were at least 25 variants of Netsky and one of the port-scanning network worm\\Sasser.\"\\\\\"Graham Cluley, senior technology consultant at Sophos, said it was staggeri ...\\\\\n",
      "3 FOAFKey: FOAF, PGP, Key Distribution, and Bloom Filters \\\\FOAF/LOAF  and bloom filters have a lot of interesting properties for social\\network and whitelist distribution.\\\\I think we can go one level higher though and include GPG/OpenPGP key\\fingerpring distribution in the FOAF file for simple web-of-trust based key\\distribution.\\\\What if we used FOAF and included the PGP key fingerprint(s) for identities?\\This could mean a lot.  You include the PGP key fingerprints within the FOAF\\file of your direct friends and then include a bloom filter of the PGP key\\fingerprints of your entire whitelist (the source FOAF file would of course need\\to be encrypted ).\\\\Your whitelist would be populated from the social network as your client\\discovered new identit ...\\\\\n",
      "3 E-mail scam targets police chief Wiltshire Police warns about \"phishing\" after its fraud squad chief was targeted.\n",
      "3 Card fraud unit nets 36,000 cards In its first two years, the UK's dedicated card fraud unit, has recovered 36,000 stolen cards and 171 arrests - and estimates it saved 65m.\n",
      "3 Group to Propose New High-Speed Wireless Format  LOS ANGELES (Reuters) - A group of technology companies  including Texas Instruments Inc. &lt;TXN.N&gt;, STMicroelectronics  &lt;STM.PA&gt; and Broadcom Corp. &lt;BRCM.O&gt;, on Thursday said they  will propose a new wireless networking standard up to 10 times  the speed of the current generation.\n",
      "3 Apple Launches Graphics Software, Video Bundle  LOS ANGELES (Reuters) - Apple Computer Inc.&lt;AAPL.O&gt; on  Tuesday began shipping a new program designed to let users  create real-time motion graphics and unveiled a discount  video-editing software bundle featuring its flagship Final Cut  Pro software.\n",
      "3 Dutch Retailer Beats Apple to Local Download Market  AMSTERDAM (Reuters) - Free Record Shop, a Dutch music  retail chain, beat Apple Computer Inc. to market on Tuesday  with the launch of a new download service in Europe's latest  battleground for digital song services.\n",
      "3 Super ant colony hits Australia A giant 100km colony of ants  which has been discovered in Melbourne, Australia, could threaten local insect species.\n",
      "3 Socialites unite dolphin groups Dolphin groups, or \"pods\", rely on socialites to keep them from collapsing, scientists claim.\n",
      "3 Teenage T. rex's monster growth Tyrannosaurus rex achieved its massive size due to an enormous growth spurt during its adolescent years.\n",
      "tensor([2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0')\n",
      "torch.Size([16, 141])\n",
      "tensor([[  870,    12,    84,  ...,     1,     1,     1],\n",
      "        [    3,   494,    22,  ...,     1,     1,     1],\n",
      "        [10892,     2,    55,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [ 2164, 29647,  7985,  ...,     1,     1,     1],\n",
      "        [53201,  7697, 14255,  ...,     1,     1,     1],\n",
      "        [ 5046,    84,     2,  ...,     1,     1,     1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list = []  # 分类标签\n",
    "    text_list = []\n",
    "    for _text, _label, _ in batch:\n",
    "        print(_label, _text)\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(truncate_pad(text_pipeline(_text), 141, vocab['<pad>']), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.stack(text_list)\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(test_map_data, batch_size=16, shuffle=False, collate_fn=collate_batch)\n",
    "for i in test_dataloader:\n",
    "    print(i[0])\n",
    "    print(i[1].shape)\n",
    "    print(i[1])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
