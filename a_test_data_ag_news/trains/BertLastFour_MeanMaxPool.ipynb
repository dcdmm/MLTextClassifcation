{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601e7994-0984-4f17-9177-6070ca9df2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, AutoConfig\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from models.BertLastFour_MeanMaxPool import BertLastFour_MeanMaxPool\n",
    "from data_ag_news.data_process.data_process_bert import Dataset, get_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ccacd5-7b54-4041-9546-49b535e6e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 10069,  2005,  1056,  1050, 11550,  2044,  7566,  9209,  5052,\n",
      "          3667,  2012,  6769,  2047,  8095,  2360,  2027,  2024,  1005,  9364,\n",
      "          1005,  2044,  7566,  2007, 16654,  6687,  3813,  2976,  9587, 24848,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  1996,  2679,  2003,  2006,  1024,  2117,  2797,  2136,  4520,\n",
      "          4888,  3058,  2005,  2529,  2686, 28968,  1006,  2686,  1012,  4012,\n",
      "          1007,  2686,  1012,  4012,  1011,  4361,  1010,  2710,  1011,  1011,\n",
      "          1037,  2117,  1032,  2136,  1997,  7596, 22862,  6637,  2005,  1996,\n",
      "          1001,  4029,  1025,  2184,  2454,  2019, 22740,  1060,  3396,  1010,\n",
      "          1037,  5049,  2005,  1032,  9139,  6787,  4942,  2953, 16313,  2389,\n",
      "          2686,  3462,  1010,  2038,  3985,  2623,  1996,  2034,  1032,  4888,\n",
      "          3058,  2005,  2049, 15371,  7596,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101, 18712,  1012,  2194,  5222,  3946,  2000,  2817, 25117,  2015,\n",
      "          1006,  9706,  1007,  9706,  1011,  1037,  2194,  2631,  2011,  1037,\n",
      "          6370, 10753,  2012,  1996,  2118,  1997, 11577,  2180,  1037,  3946,\n",
      "          2000,  4503,  1037,  4118,  1997,  5155,  2488, 25117,  2015,  1010,\n",
      "          2029,  2024,  2460,  8859,  1997, 13096, 12737,  1010,  1996,  2311,\n",
      "          5991,  1997,  8171,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101, 17547,  3131,  7126, 19939,  3748, 26332,  1006,  9706,  1007,\n",
      "          9706,  1011,  2009,  1005,  1055,  4510,  6440,  2043,  3505, 26249,\n",
      "          4627,  2010,  5670,  2007,  1037, 14819,  1997, 14231,  7341,  1010,\n",
      "          4481,  1998, 10866,  6093,  1010,  2021,  2525,  2002,  4282,  2054,\n",
      "          1996,  2154,  2097,  3288,  1012,  7407,  2097,  4894,  1999,  3182,\n",
      "          2002, 24273,  1012,  7266,  2097,  4060,  2039,  1010, 11052,  3182,\n",
      "          2097,  4318,  1998,  7311,  2097, 11950,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([2, 3, 3, 3])}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "\n",
    "token = BertTokenizer.from_pretrained(model_ckpt)\n",
    "collate_fn = get_collate_fn(token, max_len=141)\n",
    "dataset_test = Dataset('../datasets/test.jsonl')\n",
    "dataset_train = Dataset('../datasets/train.jsonl')\n",
    "\n",
    "dataLoader = Data.DataLoader(dataset=dataset_test, batch_size=4, collate_fn=collate_fn)\n",
    "for i in dataLoader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98bf942-2910-4a94-a330-de9f0b84752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertLastFour_MeanMaxPool(\n",
       "  (pretrained): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=6144, out_features=1024, bias=True)\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (linear2): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_update = AutoConfig.from_pretrained(model_ckpt)\n",
    "config_update.update({\n",
    "    \"output_hidden_states\": True\n",
    "})\n",
    "pretrained = BertModel.from_pretrained(model_ckpt, config=config_update)\n",
    "\n",
    "# 冻结网络层参数(不进行梯度更新)\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = BertLastFour_MeanMaxPool(pretrained, 4, criterion)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4917d8fe-7ff4-4dd8-b8e4-1b72ed8739b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pytorch_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9380' max='9380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9380/9380 59:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.874800</td>\n",
       "      <td>0.847427</td>\n",
       "      <td>0.898684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.845900</td>\n",
       "      <td>0.841223</td>\n",
       "      <td>0.904474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.841700</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.906579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.838200</td>\n",
       "      <td>0.835117</td>\n",
       "      <td>0.908026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.834900</td>\n",
       "      <td>0.834495</td>\n",
       "      <td>0.909079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.832900</td>\n",
       "      <td>0.834758</td>\n",
       "      <td>0.908947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.830700</td>\n",
       "      <td>0.830962</td>\n",
       "      <td>0.910921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.828400</td>\n",
       "      <td>0.830273</td>\n",
       "      <td>0.912105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.826100</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>0.913421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>0.829006</td>\n",
       "      <td>0.912632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to output_dir/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to output_dir/checkpoint-1000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to output_dir/checkpoint-1500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-1500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to output_dir/checkpoint-2000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to output_dir/checkpoint-2500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-2500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to output_dir/checkpoint-3000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to output_dir/checkpoint-3500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-3500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to output_dir/checkpoint-4000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to output_dir/checkpoint-4500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-4500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to output_dir/checkpoint-5000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to output_dir/checkpoint-5500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-5500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to output_dir/checkpoint-6000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to output_dir/checkpoint-6500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-6500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to output_dir/checkpoint-7000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to output_dir/checkpoint-7500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-7500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to output_dir/checkpoint-8000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-8000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to output_dir/checkpoint-8500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-8500/special_tokens_map.json\n",
      "Saving model checkpoint to output_dir/checkpoint-9000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in output_dir/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in output_dir/checkpoint-9000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7600\n",
      "  Batch size = 128\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9380, training_loss=0.8377859933035714, metrics={'train_runtime': 3572.1775, 'train_samples_per_second': 335.93, 'train_steps_per_second': 2.626, 'total_flos': 0.0, 'train_loss': 0.8377859933035714, 'epoch': 10.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"验证数据集评估函数\"\"\"\n",
    "    labels = pred.label_ids  # 对应自定义模型forward函数输入:labels\n",
    "    preds = pred.predictions  # 对应自定义模型forward函数返回值的第二个元素\n",
    "    preds_argmax = preds.argmax(-1)\n",
    "    f1 = f1_score(labels, preds_argmax, average='micro')\n",
    "    return {\"f1 score\": f1}  # return a dictionary string to metric value\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='output_dir',\n",
    "    overwrite_output_dir=True,\n",
    "    seed=42,\n",
    "    num_train_epochs=10.0,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy='epoch',\n",
    "    disable_tqdm=False,  # 是否使用tqdm显示进度\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=dataset_train,\n",
    "                  eval_dataset=dataset_test,\n",
    "                  data_collator=collate_fn,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=token)\n",
    "\n",
    "trainer.train()  # 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e729e9-8c68-4504-b610-a8159f341f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0886a02735466d80c36da7d0d184a8055779d3e497a063b4720b0317b8699033"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
